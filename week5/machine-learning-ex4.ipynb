{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Leaning Exercise 4: Multi-class Classification and Neural Networks\n",
    "\n",
    "From Week 5 of Coursera course, Machine Learning by Andrew Ng: https://www.coursera.org/learn/machine-learning/. The topic is back-propagation.\n",
    "\n",
    "Eric Nam, https://github.com/eric-nam, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the dataset from a MATLAB file\n",
    "The dataset contains 5,000 hand-written images in rows of a matrix (X) and labels in a vector (y). Each row is an unrolled 20 by 20 image. This data set is the same one from the previous Exercise 3 of Week 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MAT\n",
    "fpath_mat = \"../week4/ex3data1.mat\"\n",
    "file = MAT.matopen(fpath_mat)\n",
    "X = MAT.read(file, \"X\")\n",
    "y = vec(MAT.read(file, \"y\"))\n",
    "MAT.close(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 100 random examples\n",
    "The code is copied from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using StatsBase\n",
    "n = 10\n",
    "rows = sample(1:size(X)[1], n * n, replace=false)  # Sample 100 randomly\n",
    "px = isqrt(size(X)[2])\n",
    "# Combine into one big image\n",
    "image = zeros(px * n, px * n)\n",
    "for i in 0:n-1\n",
    "    for j in 0:n-1\n",
    "        image[i * px + 1: (i + 1) * px, j * px + 1: (j + 1) * px] = X[rows[i * n + j + 1], :]\n",
    "    end\n",
    "end\n",
    "heatmap(image[end:-1:1, :], c=cgrad(:grays))   # Needs to be flipped upside down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the weights of neurons (neural network parameters) from the file\n",
    "The weights are the same from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_mat = \"../week4/ex3weights.mat\"\n",
    "file = MAT.matopen(fpath_mat)\n",
    "println(MAT.names(file))\n",
    "theta1 = MAT.read(file, \"Theta1\")\n",
    "theta2 = MAT.read(file, \"Theta2\")\n",
    "MAT.close(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unroll the neural network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = vcat(theta1[:], theta2[:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    sigmoid(z)\n",
    "\n",
    "Calculate the sigmoid function, ``g(z) = \\\\frac{1}{1 + e^{-z}}``\n",
    "\n",
    "# Argument\n",
    "- `z::Number`: input variable\n",
    "\n",
    "# Return\n",
    "`Number`\n",
    "\"\"\"\n",
    "function sigmoid(z)\n",
    "    1.0 / (1.0 + exp(-z))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "Unlike the original exercise, this function does not return the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda)\n",
    "    # Recover neural network paramters\n",
    "    n_theta1 = hidden_layer_size * (input_layer_size + 1)\n",
    "    theta1 = reshape(nn_params[1:n_theta1], hidden_layer_size, input_layer_size + 1)\n",
    "    theta2 = reshape(nn_params[n_theta1+1:end], num_labels, (hidden_layer_size + 1))\n",
    "    m = size(X)[1]\n",
    "    \n",
    "    a2 = sigmoid.([ones(m, 1) X] * theta1')\n",
    "    a3 = sigmoid.([ones(size(a2)[1], 1) a2] * theta2')\n",
    "    cost = 0.\n",
    "    Y = zeros(m, num_labels)\n",
    "    Y[CartesianIndex.(1:m, convert(Array{Int32}, y))] .= 1.\n",
    "    cost = (sum((- log.(a3) .* Y .- log.(1.0 .- a3) .* (1.0 .- Y))) +\n",
    "        lambda * 0.5 * (sum(theta1[:, 2:end] .^ 2) + sum(theta2[:, 2:end] .^ 2))) / m\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some data dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost example without regularization, $\\lambda = 0$\n",
    "The expected value is 0.287629."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda = 0.\n",
    "cost_function(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost example with regularization, $\\lambda = 1$\n",
    "The expected value is 0.383770."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda = 1.\n",
    "cost_function(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sigmoid gradient\n",
    "$g'(z) = g(z)(1 - g(z))$\n",
    "\n",
    "In applications, often $g(z)$ or activation that is calculated already are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sigmoid_gradient(z)\n",
    "    sigmoid(z) * (1.0 - sigmoid.(z))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the sigmoid graident\n",
    "The gradient of sigmoid at $z=0$ is 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_gradient.([-1 -0.5 0 0.5 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network\n",
    "Codes belows tries to optimize the weights of the neural network using the backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to generate randomly initialized of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function random_initialize_weights(l_in, l_out)\n",
    "    epsilon_init = 0.12\n",
    "    theta = (2. .* rand(l_out, 1 + l_in) .- 1.) .* epsilon_init\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a gradient function of backpropagation\n",
    "$\\delta^{(i)} = (\\Theta^{(i)})^T \\delta^{(i+1)} \\cdot g'(z^{(i)})$\n",
    "\n",
    "$\\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)} (a^{(l)})^T$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\Theta^{(l)}} J(\\Theta) = D^{(l)} = \\Delta^{(l)} / m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cost_gradient(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda)\n",
    "    # Recover neural network paramters\n",
    "    n_theta1 = hidden_layer_size * (input_layer_size + 1)\n",
    "    theta1 = reshape(nn_params[1:n_theta1], hidden_layer_size, input_layer_size + 1)\n",
    "    theta2 = reshape(nn_params[n_theta1+1:end], num_labels, (hidden_layer_size + 1))\n",
    "    m = size(X)[1]\n",
    "    \n",
    "    x_aug = [ones(m, 1) X]\n",
    "    z2 = x_aug * theta1'\n",
    "    a2 = sigmoid.(z2)\n",
    "    a2_aug = [ones(size(a2)[1], 1) a2]\n",
    "    z3 = a2_aug * theta2'\n",
    "    a3 = sigmoid.(z3)\n",
    "    Y = zeros(m, num_labels)\n",
    "    Y[CartesianIndex.(1:m, convert(Array{Int32}, y))] .= 1.\n",
    "    delta3 = a3 - Y\n",
    "    theta2_gradient = mapreduce(i -> delta3[i, :] * a2_aug[i, :]', +, 1:m)\n",
    "    theta2_gradient[:, 2:end] += lambda .* theta2[:, 2:end]\n",
    "    theta2_gradient /= m\n",
    "    delta2 = delta3 * theta2 .* a2_aug .* (1. .- a2_aug)\n",
    "    theta1_gradient = mapreduce(i -> delta2[i, 2:end] * x_aug[i, :]', +, 1:m)\n",
    "    theta1_gradient[:, 2:end] += lambda .* theta1[:, 2:end]\n",
    "    theta1_gradient /= m\n",
    "    # Unroll thetas\n",
    "    vcat(theta1_gradient[:], theta2_gradient[:])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_theta1 = random_initialize_weights(input_layer_size, hidden_layer_size)\n",
    "initial_theta2 = random_initialize_weights(hidden_layer_size, num_labels)\n",
    "# Unroll parameters\n",
    "initial_theta = vcat(initial_theta1[:],  initial_theta2[:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the lambda, regularization coefficient\n",
    "lambda = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 50\n",
    "result = optimize(t -> cost_function(t, input_layer_size, hidden_layer_size, num_labels, X, y, lambda),\n",
    "                  t -> cost_gradient(t, input_layer_size, hidden_layer_size, num_labels, X, y, lambda),\n",
    "                  initial_theta, Optim.Options(iterations=50), inplace=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_opt = Optim.minimizer(result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(theta, X)\n",
    "    n_theta1 = hidden_layer_size * (input_layer_size + 1)\n",
    "    theta1 = reshape(theta[1:n_theta1], hidden_layer_size, input_layer_size + 1)\n",
    "    theta2 = reshape(theta[n_theta1+1:end], num_labels, (hidden_layer_size + 1))\n",
    "    h1 = sigmoid.([ones(size(X)[1], 1) X] *  theta1')\n",
    "    h2 = sigmoid.([ones(size(h1)[1], 1) h1] * theta2')\n",
    "    map(x -> x[2], argmax(h2, dims=2))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(theta_opt, X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy\n",
    "The accuracy would go over 95% if the model is well optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(predictions .== y) / size(X)[1] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_theta1 = hidden_layer_size * (input_layer_size + 1)\n",
    "theta1 = reshape(theta[1:n_theta1], hidden_layer_size, input_layer_size + 1)\n",
    "# Combine into one big image\n",
    "n = isqrt(hidden_layer_size)\n",
    "px = isqrt(input_layer_size)\n",
    "# Combine into one big image\n",
    "image = zeros(px * n, px * n)\n",
    "for i in 0:n-1\n",
    "    for j in 0:n-1\n",
    "        image[i * px + 1: (i + 1) * px, j * px + 1: (j + 1) * px] = theta1[i * n + j + 1, 2:end]\n",
    "    end\n",
    "end\n",
    "heatmap(image[end:-1:1, :], c=cgrad(:grays))   # Needs to be flipped upside down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
