{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Leaning Exercise 2: Logistic Regression\n",
    "From Week 3 of Coursera course, Machine Learning by Andrew Ng: https://www.coursera.org/learn/machine-learning/. The topic is the logistic regression for clustering.\n",
    "\n",
    "Eric Nam, https://github.com/eric-nam, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Part of Exercise 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the first dataset\n",
    "Load the dataset in the text file into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CSV\n",
    "using DataFrames\n",
    "fpath_csv = \"ex2data1.txt\"\n",
    "df_data1 = CSV.File(fpath_csv, header=false) |> DataFrame!;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the first dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "xlabel = \"Exam 1 Score\"\n",
    "ylabel = \"Exam 2 Score\"\n",
    "scatter(df_data1.Column1, df_data1.Column2, group=df_data1.Column3,\n",
    "    xlabel=xlabel, ylabel=ylabel, label=[\"Not admitted\" \"Admitted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a sigmoid function\n",
    "\\begin{equation}\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    sigmoid(z)\n",
    "\n",
    "Calculate the sigmoid function, ``g(z) = \\\\frac{1}{1 + e^{-z}}``\n",
    "\n",
    "# Argument\n",
    "- `z::Number`: input variable\n",
    "\n",
    "# Return\n",
    "`Number`\n",
    "\"\"\"\n",
    "function sigmoid(z)\n",
    "    1.0 / (1.0 + exp(-z))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the sigmoid function\n",
    "Zero should yield 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid.([0 100 -100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = -10:0.25:10\n",
    "plot(xx, sigmoid.(xx), label=\"Sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a cost function\n",
    "\n",
    "\\begin{align*}\n",
    "h_{\\theta} (x) &= g(\\theta ^T x) \\\\\n",
    "J(\\theta) &= \\frac{1}{m} \\sum_i^m [ -y^{(i)} \\log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)}))] \\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} &= \\frac{1}{m} \\sum_i^m ( h_{\\theta}(x^{(i)}) - y^{(i)} ) x_j^{(i)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    cost_function(theta, x, y)\n",
    "\n",
    "Compute the cost with with the dataset and theta\n",
    "\n",
    "# Arguments\n",
    "- `theta::{Number, 1}`: the coefficients of the cost function\n",
    "- `x::Array{Number, 2}` : the independent variable matrix. The rows are examples, the columns features.\n",
    "- `y::Array{Number, 1}` : the dependent vector.\n",
    "\n",
    "# Returns\n",
    "`Number`: cost\n",
    "`Number{Number, 1}`: gradient\n",
    "\"\"\"\n",
    "function cost_function(theta, x, y)\n",
    "    m, n = size(x)\n",
    "    x1 = hcat(ones(m), x)\n",
    "    sig = sigmoid.(x1 * theta)\n",
    "    cost = (- y' * log.(sig) - (1.0 .- y)' * log.(1.0 .- sig)) / m \n",
    "    grad = ((sig .- y)' * x1) / m\n",
    "    (cost, grad')\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the cost function\n",
    "The cost function is written following the function signature in the exercise instruction though it is not ideal for the optimizer package used in the exercise.\n",
    "\n",
    "With the zeto thetas, the expected cost and gradient are 0.693 and [-0.1000, -12.0092, -11.2628].\n",
    "\n",
    "With the test thetas, the expected cost and gradient are 0.218 and [0.043, 2.566, 2.647]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Matrix(df_data1[:, 1:2])\n",
    "y = df_data1[:, 3];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [0, 0, 0]\n",
    "cost_function(theta, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [-24; 0.2; 0.2]\n",
    "cost_function(theta, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the cost with an optimizer\n",
    "With zero initial coefficients, the expected cost and solution are 0.203 and [-25.161, 0.206, 0.201].\n",
    "\n",
    "Optim package is used with the default options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = optimize(t -> cost_function(t, x, y)[1], t -> cost_function(t, x, y)[2], [0., 0., 0.], inplace=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optim.minimum(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_opt = Optim.minimizer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ends = [minimum(x[:, 1]), maximum(x[:, 1])]\n",
    "y_ends = - (theta_opt[2] .* x_ends .+ theta_opt[1]) ./ theta_opt[3];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel = \"Exam 1 Score\"\n",
    "ylabel = \"Exam 2 Score\"\n",
    "scatter(x[:, 1], x[:, 2], group=y,\n",
    "    xlabel=xlabel, ylabel=ylabel, label=[\"Not admitted\" \"Admitted\"])\n",
    "plot!(x_ends, y_ends, label=\"Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Part of Exercise 2: Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_csv = \"ex2data2.txt\"\n",
    "df_data2 = CSV.File(fpath_csv, header=false) |> DataFrame!;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "xlabel = \"Microchip Test 1\"\n",
    "ylabel = \"Microchip Test 2\"\n",
    "x = Matrix(df_data2[:, 1:2])\n",
    "y = df_data2[:, 3]\n",
    "scatter(x[:, 1], x[:, 2], group=y, xlabel=xlabel, ylabel=ylabel, label=[\"y=0\" \"y=1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a feature mapping function\n",
    "Create a quadratic feature from two input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function map_feature(x1, x2)\n",
    "    degree = 6\n",
    "    vcat(1, [x1 ^ (i - j) * x2 ^ j for i in 1:degree for j in 0:i])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mapslices(r -> map_feature(r...), x, dims=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a cost and a gradient function with regularization\n",
    "\\begin{align*}\n",
    "h_{\\theta} (x) &= g(\\theta ^T x) \\\\\n",
    "J(\\theta) &= \\frac{1}{m} \\sum_i^m [ -y^{(i)} \\log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)}))] + \\frac{\\lambda}{2 m} \\sum_{j=1}^n \\theta_j^2\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0} &= \\frac{1}{m} \\sum_i^m ( h_{\\theta}(x^{(i)}) - y^{(i)} ) x_j^{(i)} \\quad \\textrm{for} \\quad j=0\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} &= \\frac{1}{m} \\sum_i^m ( h_{\\theta}(x^{(i)}) - y^{(i)} ) x_j^{(i)} + \\frac{\\lambda}{m} \\theta_j \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cost_function_reg(theta, x, y, lambda)\n",
    "    m, _ = size(x)\n",
    "    sig = sigmoid.(x * theta)\n",
    "    (- y' * log.(sig) - (1.0 .- y)' * log.(1.0 .- sig)) / m + lambda * 0.5 / m * (theta[2:end]' * theta[2:end])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cost_gradient_function_reg(theta, x, y, lambda)\n",
    "    m, n = size(x)\n",
    "    sig = sigmoid.(x * theta)\n",
    "    lambdas = fill(lambda, n)\n",
    "    lambdas[1] = 0.\n",
    "    (x' * (sig - y) + lambdas .* theta) / m\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the cost function\n",
    "With all zero $\\theta$s and $\\lambda=1$, the expected cost is 0.693.\n",
    "\n",
    "With all one $\\theta$s and $\\lambda=10$, the expected cost is 3.16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = zeros(size(features)[2])\n",
    "lambda = 1.0\n",
    "\n",
    "cost_function_reg(theta, features, y, lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = ones(size(features)[2])\n",
    "lambda = 10.\n",
    "\n",
    "cost_function_reg(theta, features, y, lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the gradient function\n",
    "With all zero $\\theta$s and $\\lambda=1$, the expected result is [.0085, 0.0188, 0.0001, 0.0503, 0.0115, ...].\n",
    "\n",
    "With all zero $\\theta$s and $\\lambda=1$, the expected result is [0.3460, 0.1614, 0.1948, 0.2269, 0.0922, ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = zeros(size(features)[2])\n",
    "lambda = 1.0\n",
    "\n",
    "cost_gradient_function_reg(theta, features, y, lambda)[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = ones(size(features)[2])\n",
    "lambda = 10.0\n",
    "\n",
    "cost_gradient_function_reg(theta, features, y, lambda)[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = zeros(size(features)[2])\n",
    "lambda = 1.\n",
    "\n",
    "result = optimize(t -> cost_function_reg(t, features, y, lambda),\n",
    "                  t -> cost_gradient_function_reg(t, features, y, lambda), theta_init, inplace=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_opt = Optim.minimizer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(minimum(x[:, 1]) - 0.2, length=50, stop=maximum(x[:, 1]) + 0.2)\n",
    "yy = range(minimum(x[:, 2]) - 0.2, length=50, stop=maximum(x[:, 2]) + 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = map(v -> theta_opt' * map_feature(v...), Iterators.product(xx, yy))\n",
    "scatter(x[:, 1], x[:, 2], group=y, xlabel=xlabel, ylabel=ylabel, label=[\"y=0\" \"y=1\"])\n",
    "contour!(xx, yy, zz', levels=[0], label=\"Decision Boundary\",\n",
    "         colorbar=nothing, color=cgrad(:rainbow), aspect_ratio=:equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface(xx, yy, zz', zlim=[-5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it again with different lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = zeros(size(features)[2])\n",
    "lambda = 100.\n",
    "\n",
    "result = optimize(t -> cost_function_reg(t, features, y, lambda),\n",
    "                  t -> cost_gradient_function_reg(t, features, y, lambda), theta_init, inplace=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_opt = Optim.minimizer(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = map(v -> theta_opt' * map_feature(v...), Iterators.product(xx, yy))\n",
    "scatter(x[:, 1], x[:, 2], group=y, xlabel=xlabel, ylabel=ylabel, label=[\"y=0\" \"y=1\"])\n",
    "contour!(xx, yy, zz', levels=[0], label=\"Decision Boundary\",\n",
    "         colorbar=nothing, color=cgrad(:rainbow), aspect_ratio=:equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface(xx, yy, zz', zlim=[-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = zeros(size(features)[2])\n",
    "lambda = 0.\n",
    "\n",
    "result = optimize(t -> cost_function_reg(t, features, y, lambda),\n",
    "                  t -> cost_gradient_function_reg(t, features, y, lambda), theta_init, inplace=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_opt = Optim.minimizer(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = map(v -> theta_opt' * map_feature(v...), Iterators.product(xx, yy))\n",
    "scatter(x[:, 1], x[:, 2], group=y, xlabel=xlabel, ylabel=ylabel, label=[\"y=0\" \"y=1\"])\n",
    "contour!(xx, yy, zz', levels=[0], label=\"Decision Boundary\",\n",
    "         colorbar=nothing, color=cgrad(:rainbow), aspect_ratio=:equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface(xx, yy, zz', zlim=[-10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
